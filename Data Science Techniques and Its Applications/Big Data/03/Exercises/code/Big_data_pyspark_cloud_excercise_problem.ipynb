{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-storage"
      ],
      "metadata": {
        "id": "ycU7XjeQhk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import storage\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/angular-yeti-389702-d5b06204e451.json'"
      ],
      "metadata": {
        "id": "1GYt_QDwhpYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "kBkY19Akh5Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbkB04J4cyqV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(\"spark.jars\", \"gcs-connector-hadoop3-latest.jar\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
        "# This is required if you are using service account and set true,\n",
        "spark._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.enable', 'true')\n",
        "spark._jsc.hadoopConfiguration().set('google.cloud.auth.service.account.json.keyfile', \"angular-yeti-389702-d5b06204e451.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from google cloud storage\n",
        "def read_file_from_cloud():\n",
        "\n",
        "  # ************* your code here **********************\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "SrmyuOgSdOAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 11\n",
        "df =  read_file_from_cloud()\n",
        "print(df.count())"
      ],
      "metadata": {
        "id": "yfY5zKeCeQkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "VNTOOVX5p7W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from google drive\n",
        "def read_file_from_drive():\n",
        "  # ************* your code here **********************\n",
        "  return data"
      ],
      "metadata": {
        "id": "mCQtYHMWnmEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 11\n",
        "df =  read_file_from_drive()\n",
        "print(df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJUaK1dobuY",
        "outputId": "70931b87-60c6-4d48-ee13-d281ba71a7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 12\n",
        "df =  read_file_from_drive()\n",
        "result = df.select(\"crim\",\"zn\",\"indus\",\"medv\").orderBy(\"medv\").toPandas()\n",
        "print(round(result.iloc[0,-1],2))"
      ],
      "metadata": {
        "id": "3O15q6gxdAbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a8e4fc-850c-4577-9de8-892158925363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 12\n",
        "df =  read_file_from_cloud()\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "query = \"\"\"\n",
        "          SELECT crim, zn, indus, medv\n",
        "          FROM df\n",
        "          order by medv\n",
        "        \"\"\"\n",
        "result = spark.sql(query).toPandas()\n",
        "print(round(result.iloc[0,-1],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snm_GkuYxTsk",
        "outputId": "b9b74c77-e7ad-4c7e-ffca-896831a0187c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "def prepare_data_using_pyspark(df):\n",
        "\n",
        "    # ************* your code here **********************\n",
        "\n",
        "\n",
        "  return final_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL4ytaNAK7Vc",
        "outputId": "239fd12d-264e-4123-9b59-b2542e3e6c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[features: vector, medv: double]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 13\n",
        "df =  read_file_from_drive() # or df =  read_file_from_cloud()\n",
        "data = prepare_data_using_pyspark(df)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBCGioC0MRU3",
        "outputId": "6bdc9ba0-8de8-497d-ad32-2790bce278ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[features: vector, medv: double]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "def train(train_data):\n",
        "  lr = LinearRegression(featuresCol=\"features\", labelCol=\"medv\", predictionCol=\"predicted_medv\")\n",
        "\n",
        "   # ************* your code here **********************\n",
        "\n",
        "  return lr_model"
      ],
      "metadata": {
        "id": "RpTUfWy5L30V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 14\n",
        "df =  read_file_from_drive() # or df =  read_file_from_cloud()\n",
        "data = prepare_data_using_pyspark(df)\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "lr_model = train(train_data)\n",
        "coefficients = lr_model.coefficients\n",
        "intercept = lr_model.intercept\n",
        "\n",
        "print(\"Coefficients: \", coefficients)\n",
        "print(\"Intercept: {:.3f}\".format(intercept))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjBs8KWOOu-y",
        "outputId": "f334ae85-e68d-4999-9826-08c39109dabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients:  [-0.11362203729409111,0.048909186934055354,0.023795428986748923,2.801771998735071,-18.415424541191168,3.5158797633118337,0.005211682161471176,-1.4163830723540525,0.3317669315937271,-0.013607893704165114,-0.953414333840846,0.008602677392852453,-0.5195035312476768]\n",
            "Intercept: 38.617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 15\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "lr_model = train(train_data)\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"predicted_medv\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
        "\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"predicted_medv\", metricName=\"r2\")\n",
        "r2 = evaluator_r2.evaluate(predictions)\n",
        "print(\"R-squared (R2) on test data: {:.3f}\".format(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuCVJBKLRboS",
        "outputId": "7ba018b6-cd1d-49b9-f9be-cd9c41c65af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data: 4.672\n",
            "R-squared (R2) on test data: 0.793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 16\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "inputCols=[\"crim\", \"zn\", \"indus\", \"chas\", \"nox\", \"rm\", \"age\", \"dis\", \"rad\", \"tax\", \"ptratio\", \"b\", \"lstat\"],\n",
        "outputCol=\"features\")\n",
        "data = assembler.transform(df)\n",
        "\n",
        "feature_importance = sorted(list(zip(data.columns[:-1], map(abs, coefficients))), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"The most important feature:\", feature_importance[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgQWT0iNRKeZ",
        "outputId": "ce9ec0c6-0887-4d38-c49c-90c9479c63d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most important feature: nox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jYpIiwKPYH5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}