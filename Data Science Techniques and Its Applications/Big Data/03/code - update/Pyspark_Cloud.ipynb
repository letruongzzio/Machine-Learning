{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ww6LMSFRuSfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71361a22-0b2c-4168-f27e-cd4c7055b071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import storage\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/southern-gasket-395903-48415c815598.json'\n"
      ],
      "metadata": {
        "id": "K7oF9si-u2ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storage_client = storage.Client()\n",
        "dir(storage_client)"
      ],
      "metadata": {
        "id": "Pqq5J08JvJ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new bucket\n",
        "bucket_name = \"aivn_testing_bucket\"\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "bucket.location = \"US\"\n",
        "bucket = storage_client.create_bucket(bucket)"
      ],
      "metadata": {
        "id": "WvLA4z9Jv2_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#access bucket\n",
        "my_bucket = storage_client.get_bucket(\"aivndemobigdata\")"
      ],
      "metadata": {
        "id": "DohVEwCKwuXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download file\n",
        "def download_file_from_bucket(blob_name, file_path, bucket_name):\n",
        "  try:\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "      storage_client.download_blob_to_file(blob,f)\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "GV9tbAIBxENA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file_from_bucket(\"asteroid/advertising.csv\", \"testing.csv\", \"aivndemobigdata\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIy9gmSxxyxN",
        "outputId": "733bdd11-e239-4074-8376-32d5dd20fa15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload file\n",
        "def upload_to_bucket(blob_name, file_path, bucket_name):\n",
        "  try:\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(file_path)\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "  return False\n",
        "\n"
      ],
      "metadata": {
        "id": "E8FsaYWjd_jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_to_bucket(\"asteroid/Movies.csv\",\"/content/BostonHousing.csv\", \"aivndemobigdata\")"
      ],
      "metadata": {
        "id": "0ri1XmbTe48O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51b16cd-e1df-4644-e629-d8d7d6897a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket = storage_client.get_bucket(\"aivndemobigdata\")\n",
        "blobs = bucket.list_blobs()\n",
        "for blob in blobs:\n",
        "    print(blob.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bicrTWOj6O_E",
        "outputId": "df451521-547e-4f72-e986-b660af396f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asteroid/\n",
            "asteroid/Movies.csv\n",
            "asteroid/advertising.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpFtax1a2Azm",
        "outputId": "eaa75899-9376-4ed8-ee92-078f36c1c0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(\"spark.jars\", \"/content/drive/MyDrive/AI2023/gcs-connector-hadoop3-latest.jar\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
        "# This is required if you are using service account and set true,\n",
        "spark._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.enable', 'true')\n",
        "spark._jsc.hadoopConfiguration().set('google.cloud.auth.service.account.json.keyfile', \"/content/southern-gasket-395903-48415c815598.json\")\n"
      ],
      "metadata": {
        "id": "v-uQKMFd708w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = \"gs://aivndemobigdata/asteroid/\"\n",
        "df = spark.read.format(\"com.databricks.spark.csv\").options(header=\"true\", inferschema=\"true\").load(input_dir+\"advertising.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQOA_VFY8K6X",
        "outputId": "035d6103-abab-493d-908a-0d86ad3949a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(TV=230.1, Radio=37.8, Newspaper=69.2, Sales=22.1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N-WXpyUdybQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.regression import GBTRegressor, RandomForestRegressor, LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "Q60x7OYj188M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o7UPbEXOHAnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs"
      ],
      "metadata": {
        "id": "gWdPvK1P8ACg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ec1baf-72a9-4583-d504-135354e5d9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (3.8.5)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2023.6.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.31.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2023.7.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (3.20.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('gcs://aivndemobigdata/asteroid/advertising.csv',\n",
        "                 storage_options={\"token\": \"/content/southern-gasket-395903-48415c815598.json\"})\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "aAKiXC--7B3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6fa8fb1-db75-412e-faa8-03beafdc30f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      TV  Radio  Newspaper  Sales\n",
            "0  230.1   37.8       69.2   22.1\n",
            "1   44.5   39.3       45.1   10.4\n",
            "2   17.2   45.9       69.3   12.0\n",
            "3  151.5   41.3       58.5   16.5\n",
            "4  180.8   10.8       58.4   17.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('gcs://aivnbigdata/asteroid/Movies.csv',\n",
        "                 storage_options={\"token\": \"/content/angular-yeti-389702-d5b06204e451.json\"})\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "Hei8x8JifbAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Big data: Small Body Dataset"
      ],
      "metadata": {
        "id": "APmlLtfPgs0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.regression import GBTRegressor, RandomForestRegressor, LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "AGxMmHL-jFbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name=\"aivnbigdata\"\n",
        "path=f\"gs://{bucket_name}/asteroid/Asteroid_Updated.csv\"\n",
        "\n",
        "df=spark.read.csv(path, sep=',', inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "YMtTtX2fg5Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preview(df, n=20):\n",
        "    return pd.DataFrame(df.take(n), columns=df.columns)\n",
        "\n",
        "preview(df)"
      ],
      "metadata": {
        "id": "A5lqRBTPhBO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"df\")\n",
        "\n",
        "query = \"\"\"\n",
        "          SELECT class, diameter, name\n",
        "          FROM df\n",
        "          WHERE diameter > 500\n",
        "        \"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "id": "0G9jRuXchJJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "astro_df = df.dropna(subset=['diameter'])\n",
        "\n",
        "astro_df = astro_df.drop('extent', 'rot_per', 'GM', 'BV', 'UB', 'IR', 'spec_B', 'spec_T', 'G', 'data_arc', 'H', 'albedo')\n",
        "\n",
        "astro_df = astro_df.withColumn('neo', regexp_replace('neo', 'Y', 'True'))\n",
        "astro_df = astro_df.withColumn('neo', regexp_replace('neo', 'N', 'False'))\n",
        "astro_df = astro_df.withColumn('pha', regexp_replace('pha', 'Y', 'True'))\n",
        "astro_df = astro_df.withColumn('pha', regexp_replace('pha', 'N', 'False'))\n",
        "astro_df = astro_df.withColumn('n_obs_used', astro_df['n_obs_used'].cast('double'))\n",
        "astro_df = astro_df.withColumn('diameter', astro_df['diameter'].cast('double'))\n",
        "\n",
        "for column in ['neo', 'pha']:\n",
        "    astro_df = astro_df.withColumn(column, astro_df[column].cast('boolean').cast('int'))\n",
        "\n",
        "astro_df = astro_df.dropna(subset=['diameter'])"
      ],
      "metadata": {
        "id": "HDmSZmM4iPOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "condition_code_indexer = StringIndexer(inputCol=\"condition_code\", outputCol=\"condition_codeIndex\")\n",
        "class_indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "onehotencoder_condition_code_vector = OneHotEncoder(inputCol=\"condition_codeIndex\", outputCol=\"condition_code_vec\")\n",
        "onehotencoder_class_vector = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"class_vec\")\n",
        "\n",
        "\n",
        "encoding_pipeline = Pipeline(stages=[condition_code_indexer,\n",
        "                            class_indexer,\n",
        "                            onehotencoder_condition_code_vector,\n",
        "                            onehotencoder_class_vector\n",
        "                    ])\n",
        "\n",
        "astro_df = encoding_pipeline.fit(astro_df).transform(astro_df)"
      ],
      "metadata": {
        "id": "tibNASWAi6Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "astro_df = astro_df.drop('condition_code', 'class', 'condition_codeIndex', 'classIndex')\n",
        "\n",
        "astro_df = astro_df.select('name', \"a\",\"e\",\"i\",'om','w','q','ad', 'per_y', 'n_obs_used', 'neo', 'pha', 'moid', 'n', 'per', 'ma', 'condition_code_vec', 'class_vec', 'diameter')\n",
        "\n",
        "features = astro_df.schema.names[1:-1]"
      ],
      "metadata": {
        "id": "rCtkuv4ijNUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = astro_df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
        "\n",
        "train_df = train.drop('name')\n",
        "val_df = val.drop('name')\n",
        "test_df = test.drop('name')"
      ],
      "metadata": {
        "id": "5L8C6LFgjPSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
        "\n",
        "test_pack = assembler.transform(test_df)\n",
        "train_pack = assembler.transform(train_df)\n",
        "val_pack = assembler.transform(val_df)\n",
        "\n",
        "for field in features:\n",
        "    test_pack = test_pack.drop(field)\n",
        "    train_pack = train_pack.drop(field)\n",
        "    val_pack = val_pack.drop(field)"
      ],
      "metadata": {
        "id": "N-hkSFXEjSMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "scaler_model = scaler.fit(train_pack)\n",
        "\n",
        "train_pack = scaler_model.transform(train_pack)\n",
        "val_pack = scaler_model.transform(val_pack)\n",
        "test_pack = scaler_model.transform(test_pack)"
      ],
      "metadata": {
        "id": "YXNhoYG8jVF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbt = GBTRegressor(featuresCol='features', labelCol='diameter', maxIter=100, maxDepth=5, seed=42, lossType='squared', stepSize=.1)\n",
        "\n",
        "gbt_model = gbt.fit(train_pack)\n",
        "gbt_pred = gbt_model.transform(val_pack)\n",
        "\n",
        "rf = RandomForestRegressor(featuresCol='features', labelCol='diameter', maxDepth=5, seed=42, bootstrap=True, numTrees=100)\n",
        "\n",
        "rf_model = rf.fit(train_pack)\n",
        "rf_pred = rf_model.transform(val_pack)\n",
        "\n",
        "lr = LinearRegression(featuresCol='features', labelCol='diameter', maxIter=100, loss='squaredError', elasticNetParam=0.5, regParam=0.1, fitIntercept=True, standardization=True, solver='auto', tol=.1)\n",
        "\n",
        "lr_model = lr.fit(train_pack)\n",
        "lr_pred = lr_model.transform(val_pack)"
      ],
      "metadata": {
        "id": "EJSidaBHjYOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = RegressionEvaluator(\n",
        "    labelCol=\"diameter\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "r2 = RegressionEvaluator(\n",
        "    labelCol=\"diameter\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "metrics = [rmse, r2]\n",
        "metric_labels = ['rmse', 'r2']\n",
        "\n",
        "predictions = [lr_pred, rf_pred, gbt_pred]\n",
        "predict_labels = ['LR', 'RF', 'GBT']\n",
        "\n",
        "eval_list = list()\n",
        "\n",
        "for pred in zip(predict_labels, predictions):\n",
        "    name = pred[0]\n",
        "    predict = pred[1]\n",
        "\n",
        "    metric_vals = pd.Series(dict([(x[0], x[1].evaluate(predict))\n",
        "                                 for x in zip(metric_labels, metrics)]),\n",
        "                            name=name)\n",
        "    eval_list.append(metric_vals)\n",
        "\n",
        "eval_df = pd.concat(eval_list, axis=1).T\n",
        "eval_df = eval_df[metric_labels]\n",
        "eval_df"
      ],
      "metadata": {
        "id": "CwPCGHv0jbBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark + SQL"
      ],
      "metadata": {
        "id": "tFBcHJo0hHwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas: read CSV file into table\n",
        "df = spark.read.option(\"header\",True).csv(\"/content/zipcodes.csv\")\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "x7n9Z6GYDHO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5110a597-3ea5-4ec2-f916-e9984ea6e05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- RecordNumber: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Zipcode: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            "\n",
            "+------------+-------+-------------------+-------+-----+\n",
            "|RecordNumber|Country|               City|Zipcode|State|\n",
            "+------------+-------+-------------------+-------+-----+\n",
            "|           1|     US|        PARC PARQUE|    704|   PR|\n",
            "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
            "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
            "|       49347|     US|               HOLT|  32564|   FL|\n",
            "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
            "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
            "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
            "|       61393|     US|           FT WORTH|  76177|   TX|\n",
            "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
            "|       76511|     US|           ASH HILL|  27007|   NC|\n",
            "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
            "|       39827|     US|               MESA|  85209|   AZ|\n",
            "|       39828|     US|               MESA|  85210|   AZ|\n",
            "|       49345|     US|           HILLIARD|  32046|   FL|\n",
            "|       49346|     US|             HOLDER|  34445|   FL|\n",
            "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
            "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
            "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
            "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
            "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
            "+------------+-------+-------------------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark: Read CSV file\n",
        "bucket_name=\"aivndemobigdata\"\n",
        "path=f\"gs://{bucket_name}/asteroid/zipcodes.csv\"\n",
        "\n",
        "spark.read.option(\"header\",True).csv(path).createOrReplaceTempView(\"Zipcodes\")"
      ],
      "metadata": {
        "id": "ArQeJoLmDe52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame API Select query\n",
        "df.select(\"country\",\"city\",\"zipcode\",\"state\").show(5)"
      ],
      "metadata": {
        "id": "QsI2dfeiE_Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c804ffd2-7cc5-44bf-bd3f-78719d805fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+-----+\n",
            "|country|               city|zipcode|state|\n",
            "+-------+-------------------+-------+-----+\n",
            "|     US|        PARC PARQUE|    704|   PR|\n",
            "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
            "|     US|       BDA SAN LUIS|    709|   PR|\n",
            "|     US|               HOLT|  32564|   FL|\n",
            "|     US|          HOMOSASSA|  34487|   FL|\n",
            "+-------+-------------------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL Select query\n",
        "spark.sql(\"SELECT country, city, zipcode, state FROM ZIPCODES\") \\\n",
        "     .show(5)"
      ],
      "metadata": {
        "id": "cyl4jLUWFT7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706e1cb4-1c45-4bea-c97d-f96f1a1e8386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+-----+\n",
            "|country|               city|zipcode|state|\n",
            "+-------+-------------------+-------+-----+\n",
            "|     US|        PARC PARQUE|    704|   PR|\n",
            "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
            "|     US|       BDA SAN LUIS|    709|   PR|\n",
            "|     US|               HOLT|  32564|   FL|\n",
            "|     US|          HOMOSASSA|  34487|   FL|\n",
            "+-------+-------------------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame API where()\n",
        "df.select(\"country\",\"city\",\"zipcode\",\"state\").\\\n",
        "                  where(\"state == 'AZ'\").show(5)"
      ],
      "metadata": {
        "id": "hcVrWx6RFhew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e41279-604b-46cf-e953-c5bb15365a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-------+-----+\n",
            "|country|city|zipcode|state|\n",
            "+-------+----+-------+-----+\n",
            "|     US|MESA|  85209|   AZ|\n",
            "|     US|MESA|  85210|   AZ|\n",
            "+-------+----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL where\n",
        "spark.sql(\"\"\" SELECT  country, city, zipcode, state FROM ZIPCODES\n",
        "          WHERE state = 'AZ' \"\"\") \\\n",
        "     .show(5)"
      ],
      "metadata": {
        "id": "eygc1JVZFwf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f72f1f9-3ac8-4728-cd1b-667b1eed9b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-------+-----+\n",
            "|country|city|zipcode|state|\n",
            "+-------+----+-------+-----+\n",
            "|     US|MESA|  85209|   AZ|\n",
            "|     US|MESA|  85210|   AZ|\n",
            "+-------+----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sorting\n",
        "df.select(\"country\",\"city\",\"zipcode\",\"state\") \\\n",
        "  .where(\"state in ('PR','AZ','FL')\") \\\n",
        "  .orderBy(\"state\") \\\n",
        "  .show(10)"
      ],
      "metadata": {
        "id": "Mi_-BoWaGGso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e428c028-ad70-4f63-87a4-e7ff180008c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+-----+\n",
            "|country|               city|zipcode|state|\n",
            "+-------+-------------------+-------+-----+\n",
            "|     US|               MESA|  85209|   AZ|\n",
            "|     US|               MESA|  85210|   AZ|\n",
            "|     US|               HOLT|  32564|   FL|\n",
            "|     US|          HOMOSASSA|  34487|   FL|\n",
            "|     US|           HILLIARD|  32046|   FL|\n",
            "|     US|             HOLDER|  34445|   FL|\n",
            "|     US|        PARC PARQUE|    704|   PR|\n",
            "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
            "|     US|       BDA SAN LUIS|    709|   PR|\n",
            "|     US|    URB EUGENE RICE|    704|   PR|\n",
            "+-------+-------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL ORDER BY\n",
        "spark.sql(\"\"\" SELECT  country, city, zipcode, state FROM ZIPCODES\n",
        "          WHERE state in ('PR','AZ','FL') order by state \"\"\") \\\n",
        "     .show(10)"
      ],
      "metadata": {
        "id": "4VB6K_phGOG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be79b5c9-baa6-4a49-f99c-274e7d15e78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------+-----+\n",
            "|country|               city|zipcode|state|\n",
            "+-------+-------------------+-------+-----+\n",
            "|     US|               MESA|  85209|   AZ|\n",
            "|     US|               MESA|  85210|   AZ|\n",
            "|     US|               HOLT|  32564|   FL|\n",
            "|     US|          HOMOSASSA|  34487|   FL|\n",
            "|     US|           HILLIARD|  32046|   FL|\n",
            "|     US|             HOLDER|  34445|   FL|\n",
            "|     US|        PARC PARQUE|    704|   PR|\n",
            "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
            "|     US|       BDA SAN LUIS|    709|   PR|\n",
            "|     US|    URB EUGENE RICE|    704|   PR|\n",
            "+-------+-------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping\n",
        "df.groupBy(\"state\").count() \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "iIM1Nzb0GWSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbaad0d-1308-4c22-8b19-d055415e13ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|state|count|\n",
            "+-----+-----+\n",
            "|   AZ|    2|\n",
            "|   NC|    3|\n",
            "|   AL|    3|\n",
            "|   TX|    3|\n",
            "|   FL|    4|\n",
            "|   PR|    5|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL GROUP BY clause\n",
        "spark.sql(\"\"\" SELECT state, count(*) as \\\n",
        "          count FROM ZIPCODES\n",
        "          GROUP BY state\"\"\") \\\n",
        "     .show()"
      ],
      "metadata": {
        "id": "JoiTYYzJGeKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea215a1-0b42-4812-9762-4dcfcf03cf49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|state|count|\n",
            "+-----+-----+\n",
            "|   AZ|    2|\n",
            "|   NC|    3|\n",
            "|   AL|    3|\n",
            "|   TX|    3|\n",
            "|   FL|    4|\n",
            "|   PR|    5|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name=\"aivnbigdata\"\n",
        "path=f\"gs://{bucket_name}/asteroid/zipcode.csv\"\n",
        "\n",
        "df=spark.read.json(path)\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "ij7PF-u3mOh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(7,False)"
      ],
      "metadata": {
        "id": "ur2wtS0Wm0L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"title\", \"price\", \"year_written\").show(5)"
      ],
      "metadata": {
        "id": "VvEBBDJFnHaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get books that are written after 1950 & cost greater than $10\n",
        "df_filtered = df.filter(\"year_written > 1950 AND price > 10 AND title IS NOT NULL\")\n",
        "df_filtered.select(\"title\", \"price\", \"year_written\").show(50, False)"
      ],
      "metadata": {
        "id": "fgl2lrb7nJlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ********* Stroke dataset"
      ],
      "metadata": {
        "id": "-4QNuBCizKh9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4Kn3ur9oDJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vn6F4PrsCwJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(\"spark.jars\", \"/content/drive/MyDrive/AI2023/gcs-connector-hadoop3-latest.jar\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
        "# This is required if you are using service account and set true,\n",
        "spark._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.enable', 'true')\n",
        "spark._jsc.hadoopConfiguration().set('google.cloud.auth.service.account.json.keyfile', \"/content/southern-gasket-395903-48415c815598.json\")\n"
      ],
      "metadata": {
        "id": "z0qA-wWXsb1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = \"gs://aivndemobigdata/asteroid/\"\n",
        "df = spark.read.format(\"com.databricks.spark.csv\").options(header=\"true\", inferschema=\"true\").load(input_dir+\"stroke_data_e.csv\")\n",
        "df.columns"
      ],
      "metadata": {
        "id": "owb-VSRwsof-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(df.take(5), columns=df.columns)\n"
      ],
      "metadata": {
        "id": "CkNnpJraosy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureColumns = ['gender','age',\n",
        " 'diabetes',\n",
        " 'hypertension',\n",
        " 'heart disease',\n",
        " 'smoking history',\n",
        " 'BMI']"
      ],
      "metadata": {
        "id": "4mp18kyp6-zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(df.age >2)\n",
        "df.count()"
      ],
      "metadata": {
        "id": "2Ln4xqx17BBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print the Type of Classes Present\n",
        "responses = df.groupBy('stroke').count().collect()\n",
        "categories = [i[0] for i in responses]\n",
        "counts = [i[1] for i in responses]\n",
        "\n",
        "ind = np.array(range(len(categories)))\n",
        "width = 0.35\n",
        "plt.bar(ind, counts, width=width, color='r')\n",
        "\n",
        "plt.ylabel('counts')\n",
        "plt.title('Stroke')\n",
        "plt.xticks(ind + width/2., categories)"
      ],
      "metadata": {
        "id": "5qpF1x-j67q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removeAllDF = df.na.drop()\n",
        "removeAllDF.describe(['BMI']).show()\n"
      ],
      "metadata": {
        "id": "BTf99Q_37VeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removeAllDF.count()\n"
      ],
      "metadata": {
        "id": "brM3mzit7Z7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputeDF = df\n",
        "imputeDF_Pandas = imputeDF.toPandas()\n"
      ],
      "metadata": {
        "id": "du5M6gx_7v8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2_9 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=2 ) & (imputeDF_Pandas['age'] <= 9)]\n",
        "values = {'smoking history': 0, 'BMI':17.125}\n",
        "df_2_9 = df_2_9.fillna(value = values)\n",
        "\n",
        "df_10_13 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=10 ) & (imputeDF_Pandas['age'] <= 13)]\n",
        "values = {'smoking history': 0, 'BMI':19.5}\n",
        "df_10_13 = df_10_13.fillna(value = values)\n",
        "\n",
        "df_14_17 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=14 ) & (imputeDF_Pandas['age'] <= 17)]\n",
        "values = {'smoking history': 0, 'BMI':23.05}\n",
        "df_14_17 = df_14_17.fillna(value = values)\n",
        "\n",
        "df_18_24 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=18 ) & (imputeDF_Pandas['age'] <= 24)]\n",
        "values = {'smoking history': 0, 'BMI':27.1}\n",
        "df_18_24 = df_18_24.fillna(value = values)\n",
        "\n",
        "df_25_29 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=25 ) & (imputeDF_Pandas['age'] <= 29)]\n",
        "values = {'smoking history': 0, 'BMI':27.9}\n",
        "df_25_29 = df_25_29.fillna(value = values)\n",
        "\n",
        "df_30_34 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=30 ) & (imputeDF_Pandas['age'] <= 34)]\n",
        "values = {'smoking history': 0.25, 'BMI':29.6}\n",
        "df_30_34 = df_30_34.fillna(value = values)\n",
        "\n",
        "df_35_44 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=35 ) & (imputeDF_Pandas['age'] <= 44)]\n",
        "values = {'smoking history': 0.25, 'BMI':30.15}\n",
        "df_35_44 = df_35_44.fillna(value = values)\n",
        "\n",
        "df_45_49 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=45 ) & (imputeDF_Pandas['age'] <= 49)]\n",
        "values = {'smoking history': 0, 'BMI':29.7}\n",
        "df_45_49 = df_45_49.fillna(value = values)\n",
        "\n",
        "df_50_59 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=50 ) & (imputeDF_Pandas['age'] <= 59)]\n",
        "values = {'smoking history': 0, 'BMI':29.95}\n",
        "df_50_59 = df_50_59.fillna(value = values)\n",
        "\n",
        "df_60_74 = imputeDF_Pandas[(imputeDF_Pandas['age'] >=60 ) & (imputeDF_Pandas['age'] <= 74)]\n",
        "values = {'smoking history': 0, 'BMI':30.1}\n",
        "df_60_74 = df_60_74.fillna(value = values)\n",
        "\n",
        "df_75_plus = imputeDF_Pandas[(imputeDF_Pandas['age'] >75 )]\n",
        "values = {'smoking history': 0, 'BMI':28.1}\n",
        "df_75_plus = df_75_plus.fillna(value = values)"
      ],
      "metadata": {
        "id": "tIPIT7Ri73Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_frames = [df_2_9, df_10_13, df_14_17, df_18_24, df_25_29, df_30_34, df_35_44, df_45_49, df_50_59, df_60_74, df_75_plus]\n",
        "df_combined = pd.concat(all_frames)"
      ],
      "metadata": {
        "id": "hJNdWxxV78vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined_converted = spark.createDataFrame(df_combined)\n",
        "imputeDF = df_combined_converted"
      ],
      "metadata": {
        "id": "00Ee4Y-I7_K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(['BMI']).show()\n",
        "imputeDF.describe(['BMI']).show()"
      ],
      "metadata": {
        "id": "qNkJiTMP8Ckv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = imputeDF.toPandas().filter(items=['gender', 'age', 'diabetes','hypertension','heart disease','smoking history','BMI'])\n",
        "Y = imputeDF.toPandas().filter(items=['stroke'])"
      ],
      "metadata": {
        "id": "WdPZqF608Ldv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n"
      ],
      "metadata": {
        "id": "5OfQuT0T8TvP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}