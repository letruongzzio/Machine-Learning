{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nGhn43DRwb9",
        "outputId": "d7914bc7-422b-4477-a4b1-887463a173e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.20\" 2023-07-18\n",
            "OpenJDK Runtime Environment (build 11.0.20+8-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.20+8-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "#Kiểm tra version java hiện tại\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tiến hành cài đặt JDK 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# -q, quiet level 2: no output except for errors\n",
        "#> /dev/null on the end of any command where you want to redirect all the stdout into nothingness"
      ],
      "metadata": {
        "id": "RNMh5l03T3bU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình sử dung java 11\n",
        "!update-alternatives --config java\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXGIAZupT8b0",
        "outputId": "f26da15f-56a9-49fc-f56d-7bc82cca90ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cấu hình javac mặc định (chọn số 2)\n",
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1md6yULVo63",
        "outputId": "c3af7529-e0c9-4324-cde3-938ce2997c1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cấu hình jps (Java process status) (choose option 2)\n",
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if21Snp0WORI",
        "outputId": "9929157f-2ee7-4f02-d4c4-99fb8c17cadf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xem version java hiện tại\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d6d5HhsW2Fe",
        "outputId": "4c9902bd-7cbf-4d98-9042-578d51b01aa4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_382\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_382-8u382-ga-1~22.04.1-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Java related environment variables\n",
        "#The JAVA_HOME is an operating system environment variable points to the file system location where the JDK or JRE was installed.\n",
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urZ23SGtW9LC",
        "outputId": "e3b26a27-196b-4647-8ff9-7b962d2c512f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zB0Y1_QFdmaP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing os module\n",
        "import os\n",
        "#Creating environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":JAVA_HOME/bin:JRE_HOME/bin:HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "3kAZwqqyXN2R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Secure Shell Server (SSHD)\n",
        "We need to define a means for the master node to remotely access every node in our cluster.\n",
        "\n",
        "Hadoop uses passphrases SSH for the communication between the nodes.\n",
        "\n",
        "SSH is a cryptographic network protocol for operating network services securely over an unsecured network.\n",
        "\n",
        "SSH utilizes standard public key criptography to create a pair of keys for user verification: one public and one private."
      ],
      "metadata": {
        "id": "8_m4V5U4XlvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing openssh-server\n",
        "!apt-get install openssh-server -qq > /dev/null"
      ],
      "metadata": {
        "id": "H3QMT_tEXniF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg2QVb7fX7FP",
        "outputId": "ddd2f2a8-55d0-4e36-8e8c-9a12f3cc42c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep Port /etc/ssh/sshd_config\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nin5al4wX9om",
        "outputId": "6131d63c-ccf0-4716-df78-bd3f8dce3bd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Port 22\n",
            "#GatewayPorts no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlrdFB_3YDRx",
        "outputId": "9904cd6f-73f9-4d00-d888-263c17bfbe8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:dz8zNm5GNTSWNEEWHQE3gNjnTzNF2QnuVCi7zUAihNs root@ccb14c0b25d0\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|      o.  o .+B#%|\n",
            "|     . . o =.o+B*|\n",
            "|      o . o =oo o|\n",
            "|     . E   oo. =.|\n",
            "|        S . *.o.+|\n",
            "|         . o +.. |\n",
            "|             .B  |\n",
            "|             oo= |\n",
            "|             o.  |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing the public key\n",
        "!more /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhDc8muHYGfh",
        "outputId": "b98ad617-d9f0-4207-910b-03579b72004e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDhesZbtFQ9A9u/EyqAvgJTe1GiEK2w6NX0k3Rq5Z9O\n",
            "6o21rxmq3KiMfu8tz619mObnfa+0FBc+wphtNG44w2MfJ9MF4sWVb0CV70Pxs5Jt6P4NMNgSEm8nksIu\n",
            "j6cIiqorcmZklFouuX0qoEtzdlhXR0YtpXwprqx1uTSZJJsHe3SqKZdTOv9Yt1yOtkHFM5Z1UbNjxWLz\n",
            "eImnc2Z/9TuLaeN9qN4mHpGwr7wo1YRF0//2pVwBKUlWzeleY5Q3nCOAabbGFjB6lk/XAIgWO9F7WnoZ\n",
            "ZRf48uyussg3hKYL7qCIMdrs1Kg/TmZGpa1hjPZI6rZl9kSB+sXaqq62zQUg1fuEOm7echevL11ElYoJ\n",
            "YRgD4AcNID8yy5uiZ7OIlQNTNVwVTjn9NveJQ/8eD96Oou3ENE/VPzebKPQ17m0aLRv/TvgjCrbU4SaQ\n",
            "zhnNwQOGfVe9d2n0KYIlR0jOhVjOJVNbVKUFy1Lpk7oTJkcdlTa9KaU/KYwaoOwQdnJ5dkc= root@cc\n",
            "b14c0b25d0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "xEgG03IxYK3A"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akFZCv4wYh97",
        "outputId": "2e4326fc-1be5-4317-ed95-cd46379037aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            " 12:20:38 up 4 min,  0 users,  load average: 1.66, 0.83, 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "id": "k4kketFLYrUF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "id": "QY7K0p0SYtnG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively"
      ],
      "metadata": {
        "id": "pvZsBc93YwN1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop directory\n",
        "!ls /usr/local/hadoop-3.2.3/etc/hadoop\n",
        "#we can see various configuration files of hadoop\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4XDhe3YY8PD",
        "outputId": "775a9f2c-5392-4103-a070-60ff5c3d6299"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  kms-log4j.properties\n",
            "configuration.xsl\t\t  kms-site.xml\n",
            "container-executor.cfg\t\t  log4j.properties\n",
            "core-site.xml\t\t\t  mapred-env.cmd\n",
            "hadoop-env.cmd\t\t\t  mapred-env.sh\n",
            "hadoop-env.sh\t\t\t  mapred-queues.xml.template\n",
            "hadoop-metrics2.properties\t  mapred-site.xml\n",
            "hadoop-policy.xml\t\t  shellprofile.d\n",
            "hadoop-user-functions.sh.example  ssl-client.xml.example\n",
            "hdfs-site.xml\t\t\t  ssl-server.xml.example\n",
            "httpfs-env.sh\t\t\t  user_ec_policies.xml.template\n",
            "httpfs-log4j.properties\t\t  workers\n",
            "httpfs-signature.secret\t\t  yarn-env.cmd\n",
            "httpfs-site.xml\t\t\t  yarn-env.sh\n",
            "kms-acls.xml\t\t\t  yarnservice-log4j.properties\n",
            "kms-env.sh\t\t\t  yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-env.sh file\n",
        "!cat /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT_45sCrZDKB",
        "outputId": "076c830d-fb7e-4f92-bc5a-ff72f7538085"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-openstack,hadoop-azure-datalake,hadoop-aliyun,hadoop-aws,hadoop-azure\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "d--2hm7SZcZp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Hadoop home variable\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ],
      "metadata": {
        "id": "nm44x5mBZiuU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop xml files\n",
        "!ls $HADOOP_HOME/etc/hadoop/*.xml\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07_7q0XzZsug",
        "outputId": "190d9c68-2ee1-46d7-db40-c5457d90a5de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/core-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hadoop-policy.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hdfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/httpfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-acls.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/mapred-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6s5sgFRaTBS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of core-site.xml file\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Z0aPv1Z6gd",
        "outputId": "4b142a6c-653f-4944-caeb-4f3b48eceb1c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ## Running Hadoop in standalone mode\n"
      ],
      "metadata": {
        "id": "qNsVtMj1aM3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring mapreduce tools\n",
        "!ls $HADOOP_HOME/share/hadoop/mapreduce/*.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3RocK0oaJTY",
        "outputId": "1d754da3-585b-4e9a-fb2b-458a810b55e3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the examples of programs available\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqIVa6Ila41p",
        "outputId": "32bfc8cb-a242-4788-9192-af28a37e3c77"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example program must be given as the first argument.\n",
            "Valid program names are:\n",
            "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
            "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
            "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
            "  dbcount: An example job that count the pageview counts from a database.\n",
            "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
            "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
            "  join: A job that effects a join over sorted, equally partitioned datasets\n",
            "  multifilewc: A job that counts words from several files.\n",
            "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
            "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
            "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
            "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
            "  secondarysort: An example defining a secondary sort to the reduce.\n",
            "  sort: A map/reduce program that sorts the data written by the random writer.\n",
            "  sudoku: A sudoku solver.\n",
            "  teragen: Generate data for the terasort\n",
            "  terasort: Run the terasort\n",
            "  teravalidate: Checking results of terasort\n",
            "  wordcount: A map/reduce program that counts the words in the input files.\n",
            "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
            "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
            "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Usage of the wordcount MapReduce program\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meCOPwnybC61",
        "outputId": "9e8aca47-9c02-4d47-8924-3491a6fd05f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: wordcount <in> [<in>...] <out>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "gDkN0pJvbHyz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce program wordcount\n",
        "#the output directory will be created automatically\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /content/101.txt /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhtyyrlUbPy1",
        "outputId": "f1986169-63e5-4c26-97a2-a5955ce33aa0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-04 12:21:33,098 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-08-04 12:21:33,229 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-08-04 12:21:33,229 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-08-04 12:21:33,423 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2023-08-04 12:21:33,474 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-08-04 12:21:33,674 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1552352151_0001\n",
            "2023-08-04 12:21:33,674 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-08-04 12:21:33,923 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-08-04 12:21:33,924 INFO mapreduce.Job: Running job: job_local1552352151_0001\n",
            "2023-08-04 12:21:33,932 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-08-04 12:21:33,941 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-08-04 12:21:33,941 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-08-04 12:21:33,942 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2023-08-04 12:21:33,987 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-08-04 12:21:33,988 INFO mapred.LocalJobRunner: Starting task: attempt_local1552352151_0001_m_000000_0\n",
            "2023-08-04 12:21:34,023 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-08-04 12:21:34,023 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-08-04 12:21:34,052 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-08-04 12:21:34,058 INFO mapred.MapTask: Processing split: file:/content/101.txt:0+678064\n",
            "2023-08-04 12:21:34,195 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-08-04 12:21:34,195 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-08-04 12:21:34,195 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-08-04 12:21:34,196 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-08-04 12:21:34,196 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-08-04 12:21:34,203 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-08-04 12:21:34,451 INFO mapred.LocalJobRunner: \n",
            "2023-08-04 12:21:34,451 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-08-04 12:21:34,451 INFO mapred.MapTask: Spilling map output\n",
            "2023-08-04 12:21:34,451 INFO mapred.MapTask: bufstart = 0; bufend = 1078906; bufvoid = 104857600\n",
            "2023-08-04 12:21:34,451 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25792448(103169792); length = 421949/6553600\n",
            "2023-08-04 12:21:34,930 INFO mapreduce.Job: Job job_local1552352151_0001 running in uber mode : false\n",
            "2023-08-04 12:21:34,931 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-08-04 12:21:35,255 INFO mapred.MapTask: Finished spill 0\n",
            "2023-08-04 12:21:35,271 INFO mapred.Task: Task:attempt_local1552352151_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-08-04 12:21:35,274 INFO mapred.LocalJobRunner: map\n",
            "2023-08-04 12:21:35,274 INFO mapred.Task: Task 'attempt_local1552352151_0001_m_000000_0' done.\n",
            "2023-08-04 12:21:35,284 INFO mapred.Task: Final Counters for attempt_local1552352151_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=994673\n",
            "\t\tFILE: Number of bytes written=1181343\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tSpilled Records=21438\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "2023-08-04 12:21:35,284 INFO mapred.LocalJobRunner: Finishing task: attempt_local1552352151_0001_m_000000_0\n",
            "2023-08-04 12:21:35,285 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-08-04 12:21:35,291 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-08-04 12:21:35,291 INFO mapred.LocalJobRunner: Starting task: attempt_local1552352151_0001_r_000000_0\n",
            "2023-08-04 12:21:35,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-08-04 12:21:35,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-08-04 12:21:35,305 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-08-04 12:21:35,310 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2593e862\n",
            "2023-08-04 12:21:35,312 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-08-04 12:21:35,365 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-08-04 12:21:35,392 INFO reduce.EventFetcher: attempt_local1552352151_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-08-04 12:21:35,438 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1552352151_0001_m_000000_0 decomp: 318001 len: 318005 to MEMORY\n",
            "2023-08-04 12:21:35,442 INFO reduce.InMemoryMapOutput: Read 318001 bytes from map-output for attempt_local1552352151_0001_m_000000_0\n",
            "2023-08-04 12:21:35,444 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318001, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318001\n",
            "2023-08-04 12:21:35,446 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-08-04 12:21:35,447 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-08-04 12:21:35,447 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-08-04 12:21:35,465 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-08-04 12:21:35,468 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 317994 bytes\n",
            "2023-08-04 12:21:35,532 INFO reduce.MergeManagerImpl: Merged 1 segments, 318001 bytes to disk to satisfy reduce memory limit\n",
            "2023-08-04 12:21:35,546 INFO reduce.MergeManagerImpl: Merging 1 files, 318005 bytes from disk\n",
            "2023-08-04 12:21:35,547 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-08-04 12:21:35,548 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-08-04 12:21:35,558 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 317994 bytes\n",
            "2023-08-04 12:21:35,559 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-08-04 12:21:35,565 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-08-04 12:21:35,935 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-08-04 12:21:35,937 INFO mapred.Task: Task:attempt_local1552352151_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-08-04 12:21:35,942 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-08-04 12:21:35,942 INFO mapred.Task: Task attempt_local1552352151_0001_r_000000_0 is allowed to commit now\n",
            "2023-08-04 12:21:35,945 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1552352151_0001_r_000000_0' to file:/content/output\n",
            "2023-08-04 12:21:35,949 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2023-08-04 12:21:35,949 INFO mapred.Task: Task 'attempt_local1552352151_0001_r_000000_0' done.\n",
            "2023-08-04 12:21:35,950 INFO mapred.Task: Final Counters for attempt_local1552352151_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1630715\n",
            "\t\tFILE: Number of bytes written=1734820\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=21438\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=57\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=235472\n",
            "2023-08-04 12:21:35,950 INFO mapred.LocalJobRunner: Finishing task: attempt_local1552352151_0001_r_000000_0\n",
            "2023-08-04 12:21:35,953 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-08-04 12:21:36,940 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-08-04 12:21:36,940 INFO mapreduce.Job: Job job_local1552352151_0001 completed successfully\n",
            "2023-08-04 12:21:36,962 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2625388\n",
            "\t\tFILE: Number of bytes written=2916163\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=57\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=235472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Running Hadoop in Pseudo-distributed mode"
      ],
      "metadata": {
        "id": "Upd7ginpbd8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>fs.defaultFS</name>\\n\\\n",
        "    <value>hdfs://localhost:9000</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "id": "_LgxOmLRbfsz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of core-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OfpOX5_bzT_",
        "outputId": "ebf48697-684f-47c8-86f5-f11122a70f42"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>fs.defaultFS</name>\n",
            "     <value>hdfs://localhost:9000</value>\n",
            "   </property>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to hdfs-site.xml file\n",
        "#Since we are running Hadoop in only one machine, a replication factor greater than 1 does not make sense\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>dfs.replication</name>\\n\\\n",
        "    <value>1</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "GXpedQMqcN2r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of hdfs-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhdP0rOBcQHu",
        "outputId": "2dbe4dc2-539f-4c09-e416-c4a7eea6fd78"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>dfs.replication</name>\n",
            "     <value>1</value>\n",
            "   </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to mapred-site.xml file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.framework.name</name>\\n\\\n",
        "    <value>yarn</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.application.classpath</name>\\n\\\n",
        "    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "id": "s-z62huIcYEZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of mapred-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxirlNeXcaW8",
        "outputId": "50eedc5c-8a7c-47d2-d54d-ca6b625cc3b5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>mapreduce.framework.name</name>\n",
            "     <value>yarn</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>mapreduce.application.classpath</name>\n",
            "     <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\n",
            "   </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to yarn-site.xml file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <description>The hostname of the RM.</description>\\n\\\n",
        "    <name>yarn.resourcemanager.hostname</name>\\n\\\n",
        "    <value>localhost</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.aux-services</name>\\n\\\n",
        "    <value>mapreduce_shuffle</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\\n\\\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "phInV9NHchku"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of yarn-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBvPbqfYcjk6",
        "outputId": "a97b0536-932a-40aa-c07a-ac245bb78ec6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "<configuration>\n",
            "<property>\n",
            "     <description>The hostname of the RM.</description>\n",
            "     <name>yarn.resourcemanager.hostname</name>\n",
            "     <value>localhost</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>yarn.nodemanager.aux-services</name>\n",
            "     <value>mapreduce_shuffle</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>yarn.nodemanager.env-whitelist</name>\n",
            "     <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
            "   </property>\n",
            "\n",
            "<!-- Site specific YARN configuration properties -->\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3UNcJ1rcqfs",
        "outputId": "9b2dde64-3788-4daf-91dd-27af016bab93"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2023-08-04 12:21:38,778 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = ccb14c0b25d0/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_382\n",
            "************************************************************/\n",
            "2023-08-04 12:21:38,817 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2023-08-04 12:21:38,967 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-2c5d657e-3e4e-40e7-ad51-30cdc8f5dbf1\n",
            "2023-08-04 12:21:39,788 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2023-08-04 12:21:39,840 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2023-08-04 12:21:39,842 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2023-08-04 12:21:39,843 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2023-08-04 12:21:39,851 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2023-08-04 12:21:39,851 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2023-08-04 12:21:39,851 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2023-08-04 12:21:39,852 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2023-08-04 12:21:39,922 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2023-08-04 12:21:39,937 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2023-08-04 12:21:39,937 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2023-08-04 12:21:39,947 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2023-08-04 12:21:39,948 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Aug 04 12:21:39\n",
            "2023-08-04 12:21:39,950 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2023-08-04 12:21:39,950 INFO util.GSet: VM type       = 64-bit\n",
            "2023-08-04 12:21:39,951 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2023-08-04 12:21:39,952 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2023-08-04 12:21:39,978 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2023-08-04 12:21:39,978 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2023-08-04 12:21:40,006 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2023-08-04 12:21:40,006 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2023-08-04 12:21:40,006 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2023-08-04 12:21:40,007 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2023-08-04 12:21:40,052 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2023-08-04 12:21:40,052 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2023-08-04 12:21:40,052 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2023-08-04 12:21:40,052 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2023-08-04 12:21:40,079 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2023-08-04 12:21:40,079 INFO util.GSet: VM type       = 64-bit\n",
            "2023-08-04 12:21:40,080 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2023-08-04 12:21:40,080 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2023-08-04 12:21:40,083 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2023-08-04 12:21:40,083 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2023-08-04 12:21:40,083 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2023-08-04 12:21:40,084 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2023-08-04 12:21:40,091 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2023-08-04 12:21:40,097 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2023-08-04 12:21:40,105 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2023-08-04 12:21:40,105 INFO util.GSet: VM type       = 64-bit\n",
            "2023-08-04 12:21:40,105 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2023-08-04 12:21:40,105 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2023-08-04 12:21:40,123 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2023-08-04 12:21:40,124 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2023-08-04 12:21:40,124 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2023-08-04 12:21:40,134 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2023-08-04 12:21:40,134 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2023-08-04 12:21:40,136 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2023-08-04 12:21:40,136 INFO util.GSet: VM type       = 64-bit\n",
            "2023-08-04 12:21:40,137 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2023-08-04 12:21:40,137 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2023-08-04 12:21:40,176 INFO namenode.FSImage: Allocated new BlockPoolId: BP-527601081-172.28.0.12-1691151700166\n",
            "2023-08-04 12:21:40,208 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2023-08-04 12:21:40,255 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2023-08-04 12:21:40,472 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .\n",
            "2023-08-04 12:21:40,496 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2023-08-04 12:21:40,543 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2023-08-04 12:21:40,544 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2023-08-04 12:21:40,549 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2023-08-04 12:21:40,549 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at ccb14c0b25d0/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop scripts available in sbin directory\n",
        "!ls $HADOOP_HOME/sbin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlA73jXSdnpT",
        "outputId": "1a700d48-2976-4f23-ccd5-02e2ebd77e2b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribute-exclude.sh\t start-all.sh\t      stop-balancer.sh\n",
            "FederationStateStore\t start-balancer.sh    stop-dfs.cmd\n",
            "hadoop-daemon.sh\t start-dfs.cmd\t      stop-dfs.sh\n",
            "hadoop-daemons.sh\t start-dfs.sh\t      stop-secure-dns.sh\n",
            "httpfs.sh\t\t start-secure-dns.sh  stop-yarn.cmd\n",
            "kms.sh\t\t\t start-yarn.cmd       stop-yarn.sh\n",
            "mr-jobhistory-daemon.sh  start-yarn.sh\t      workers.sh\n",
            "refresh-namenodes.sh\t stop-all.cmd\t      yarn-daemon.sh\n",
            "start-all.cmd\t\t stop-all.sh\t      yarn-daemons.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "w5aqncusds8a"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SFfNp7zdvJu",
        "outputId": "e63381c2-6013-4cc0-ee7f-f3cb1cd947e5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [ccb14c0b25d0]\n",
            "ccb14c0b25d0: Warning: Permanently added 'ccb14c0b25d0' (ED25519) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT1DRU03d3ox",
        "outputId": "a8825eef-aba0-4555-b5d7-6be761042478"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2469 Jps\n",
            "2106 DataNode\n",
            "2302 SecondaryNameNode\n",
            "1998 NameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC8PwHa2d-L0",
        "outputId": "64b5f4c5-075d-4f6b-f211-f5a3ad1fbf6b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: ignoring input and appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-WNfP1KeDMY",
        "outputId": "cde8b121-e333-4eaa-b96b-636aefaf0e1e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2673 NodeManager\n",
            "2106 DataNode\n",
            "2570 ResourceManager\n",
            "2795 Jps\n",
            "2302 SecondaryNameNode\n",
            "1998 NameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJlPfFDveFw8",
        "outputId": "8552d529-5abe-4ae9-d7f8-a2a4f6506da0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 86902976512 (80.93 GB)\n",
            "DFS Remaining: 86902951936 (80.93 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: ccb14c0b25d0\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "Non DFS Used: 28738437120 (26.76 GB)\n",
            "DFS Remaining: 86902951936 (80.93 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 75.14%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 1\n",
            "Last contact: Fri Aug 04 12:22:18 UTC 2023\n",
            "Last Block Report: Fri Aug 04 12:21:58 UTC 2023\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n"
      ],
      "metadata": {
        "id": "hOuDefi8eL6M"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The namenode posts the general report on port 9870\n",
        "output.serve_kernel_port_as_window(9870)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WJ_Y-RM2eRR-",
        "outputId": "d3aae07d-4f82-4982-8745-d0d712023835"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(9870, \"/\", \"https://localhost:9870/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "-dqJP_6reWYO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count"
      ],
      "metadata": {
        "id": "pCwJSseveZdC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNoRxNuTeidJ",
        "outputId": "161d7c46-eba7-4835-ee76-be1ae6443d9a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     678064 2023-08-04 12:22 /word_count/101.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce program wordcount\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/101.txt /word_count/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6gLjeYgep97",
        "outputId": "a7320e59-6e37-499c-d4bd-ddfaa6b3ea10"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-04 12:22:36,026 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-08-04 12:22:36,800 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1691151736311_0001\n",
            "2023-08-04 12:22:37,227 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2023-08-04 12:22:37,396 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-08-04 12:22:37,650 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1691151736311_0001\n",
            "2023-08-04 12:22:37,653 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-08-04 12:22:37,940 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-08-04 12:22:37,941 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-08-04 12:22:38,787 INFO impl.YarnClientImpl: Submitted application application_1691151736311_0001\n",
            "2023-08-04 12:22:38,847 INFO mapreduce.Job: The url to track the job: http://ccb14c0b25d0:8088/proxy/application_1691151736311_0001/\n",
            "2023-08-04 12:22:38,848 INFO mapreduce.Job: Running job: job_1691151736311_0001\n",
            "2023-08-04 12:22:52,255 INFO mapreduce.Job: Job job_1691151736311_0001 running in uber mode : false\n",
            "2023-08-04 12:22:52,256 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-08-04 12:23:01,459 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-08-04 12:23:07,531 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-08-04 12:23:08,559 INFO mapreduce.Job: Job job_1691151736311_0001 completed successfully\n",
            "2023-08-04 12:23:08,707 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318005\n",
            "\t\tFILE: Number of bytes written=1108657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=678169\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=8\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=1\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=1\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=6649\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=4031\n",
            "\t\tTotal time spent by all map tasks (ms)=6649\n",
            "\t\tTotal time spent by all reduce tasks (ms)=4031\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=6649\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=4031\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=6808576\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4127744\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=264\n",
            "\t\tCPU time spent (ms)=4160\n",
            "\t\tPhysical memory (bytes) snapshot=519581696\n",
            "\t\tVirtual memory (bytes) snapshot=5082214400\n",
            "\t\tTotal committed heap usage (bytes)=534249472\n",
            "\t\tPeak Map Physical memory (bytes)=340672512\n",
            "\t\tPeak Map Virtual memory (bytes)=2540511232\n",
            "\t\tPeak Reduce Physical memory (bytes)=178909184\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2541703168\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZXydaGe_aO",
        "outputId": "ff2585e2-4760-4dab-adf9-136ac772a41c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-08-04 12:23 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup     233636 2023-08-04 12:23 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | head -50\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdAxLe10gK4K",
        "outputId": "774279dd-aac1-4995-ebb3-4507468d5685"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"(d)\t1\n",
            "\"/H\"\t1\n",
            "\"0\"\t1\n",
            "\"02\"\t2\n",
            "\"1.\t1\n",
            "\"414\t3\n",
            "\"A\t2\n",
            "\"AT&T\t3\n",
            "\"AT&T's\t1\n",
            "\"Access\t1\n",
            "\"Acid\t3\n",
            "\"Ad-hocracy\"\t1\n",
            "\"Advanced\t1\n",
            "\"Agents\t1\n",
            "\"Al\t3\n",
            "\"All\t3\n",
            "\"American\t1\n",
            "\"An\t1\n",
            "\"And\t2\n",
            "\"Any\t1\n",
            "\"Are\t3\n",
            "\"Artificial\t1\n",
            "\"As\t1\n",
            "\"Assistant\t1\n",
            "\"Attctc\"\t2\n",
            "\"Auld\t1\n",
            "\"Autodesk,\"\t1\n",
            "\"BBS,\"\t2\n",
            "\"BIRTHPLACE\t1\n",
            "\"BRITS\t1\n",
            "\"Barry\t1\n",
            "\"Because\t1\n",
            "\"Before\t1\n",
            "\"Bell\t2\n",
            "\"Bell\"\t1\n",
            "\"BellSouth\t2\n",
            "\"Berkeley\t1\n",
            "\"Big\t1\n",
            "\"Biggest\t1\n",
            "\"Black\t3\n",
            "\"Blue\t1\n",
            "\"Bob\"\t1\n",
            "\"Bob,\t3\n",
            "\"Bob:\t1\n",
            "\"Bullet-N-Board.\"\t1\n",
            "\"Bureaucrat-ese\t1\n",
            "\"But\t4\n",
            "\"C-word.\"\t1\n",
            "\"CALIFORNIA\"\t1\n",
            "\"CC,\"\t1\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Hadoop Streaming Using Python\n",
        "\n",
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydXjC8rCgMmY",
        "outputId": "fe937fef-7424-4c35-d7ab-f194cc7c8d94"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.3.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.3.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.3.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.3.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.3.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.3.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.3.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.3.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.3.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.3.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.3.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.3.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.3.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.3.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.3.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.3.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.3.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "PHtgWAwzgWGf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python\n",
        ""
      ],
      "metadata": {
        "id": "cpyVD9lygaD-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "KKUCZvjqgc5S"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # split the line into words\n",
        "  words = line.split()\n",
        "\n",
        "  # we are looping over the words array and printing the word\n",
        "  # with the count of 1 to the STDOUT\n",
        "  for word in words:\n",
        "    # write the results to STDOUT (standard output);\n",
        "    # what we output here will be the input for the\n",
        "    # Reduce step, i.e. the input for reducer.py\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukCUHeo0ghim",
        "outputId": "932ea9d3-ba67-4fe6-9c71-83d8827e2acb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3nDlOl9goN8",
        "outputId": "69c60495-2d75-4b42-912b-bc3f33bb94d3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat 101.txt | python mapper.py | sort -k1,1 | python reducer.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxL3LDfogssN",
        "outputId": "ad02c203-bc36-44e1-dc83-474a490aa454"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\t31\n",
            "##\t2\n",
            "##!\t1\n",
            "&\t10\n",
            "(.\t8\n",
            "**\t4\n",
            "***\t8\n",
            "*****\t2\n",
            "+\t2\n",
            "+------------+\t2\n",
            "-\t39\n",
            "-----------\t1\n",
            "-------------\t1\n",
            "---------------\t1\n",
            "----------------------\t1\n",
            "----------------------------\t1\n",
            "---------------------------------\t1\n",
            "-----------------------------------\t1\n",
            ".\t271\n",
            ".\"\t9\n",
            ".'\t1\n",
            ".)\t8\n",
            ":\t1\n",
            "=\t2\n",
            "@\t1\n",
            "_____________________________________\t1\n",
            "~~~~~~~~~~~~~~\t1\n",
            "~~~~~~~~~~~~~~~~~~~~~\t2\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t1\n",
            "($1\t1\n",
            "$10,000\t1\n",
            "$1.2\t1\n",
            "$129.\t1\n",
            "$13,\t1\n",
            "$17,099.\t1\n",
            "$1877.20).\t1\n",
            "$20.\t1\n",
            "$233,000\t1\n",
            "$233,846.14,\t1\n",
            "$233,880\t2\n",
            "$24,639.05--based,\t1\n",
            "$25\t1\n",
            "$250,000--through\t1\n",
            "$252.36.\t1\n",
            "$27\t1\n",
            "$300,000\t1\n",
            "$31\t1\n",
            "$313.\t1\n",
            "$35\t1\n",
            "$367.\t1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/reducer.py\", line 32, in <module>\n",
            "    print('%s\\t%s' % (current_word, current_count))\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper.py /content/reducer.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users\n",
        ""
      ],
      "metadata": {
        "id": "4qjAvebEgzdw"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /word_count_with_python/101.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxEQ2aW8g1uB",
        "outputId": "54d73d8e-d898-4c4d-b116-1dafa02a9d35"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar3674268459469342600/] [] /tmp/streamjob4964430483085602908.jar tmpDir=null\n",
            "2023-08-04 12:26:25,897 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-08-04 12:26:26,891 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-08-04 12:26:27,994 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1691151736311_0002\n",
            "2023-08-04 12:26:29,092 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-08-04 12:26:29,395 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-08-04 12:26:30,016 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1691151736311_0002\n",
            "2023-08-04 12:26:30,018 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-08-04 12:26:30,635 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-08-04 12:26:30,636 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-08-04 12:26:31,009 INFO impl.YarnClientImpl: Submitted application application_1691151736311_0002\n",
            "2023-08-04 12:26:31,143 INFO mapreduce.Job: The url to track the job: http://ccb14c0b25d0:8088/proxy/application_1691151736311_0002/\n",
            "2023-08-04 12:26:31,167 INFO mapreduce.Job: Running job: job_1691151736311_0002\n",
            "2023-08-04 12:26:41,663 INFO mapreduce.Job: Job job_1691151736311_0002 running in uber mode : false\n",
            "2023-08-04 12:26:41,666 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-08-04 12:26:51,962 INFO mapreduce.Job:  map 50% reduce 0%\n",
            "2023-08-04 12:26:52,971 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-08-04 12:27:00,047 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-08-04 12:27:00,068 INFO mapreduce.Job: Job job_1691151736311_0002 completed successfully\n",
            "2023-08-04 12:27:00,184 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1078912\n",
            "\t\tFILE: Number of bytes written=2873182\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=682368\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=16524\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=5241\n",
            "\t\tTotal time spent by all map tasks (ms)=16524\n",
            "\t\tTotal time spent by all reduce tasks (ms)=5241\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=16524\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=5241\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=16920576\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5366784\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=867930\n",
            "\t\tMap output materialized bytes=1078918\n",
            "\t\tInput split bytes=208\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=1078918\n",
            "\t\tReduce input records=105488\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=210976\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=609\n",
            "\t\tCPU time spent (ms)=5330\n",
            "\t\tPhysical memory (bytes) snapshot=878706688\n",
            "\t\tVirtual memory (bytes) snapshot=7627800576\n",
            "\t\tTotal committed heap usage (bytes)=869793792\n",
            "\t\tPeak Map Physical memory (bytes)=335196160\n",
            "\t\tPeak Map Virtual memory (bytes)=2549313536\n",
            "\t\tPeak Reduce Physical memory (bytes)=220725248\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2542010368\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=682160\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n",
            "2023-08-04 12:27:00,184 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_CM_6TUhOQH",
        "outputId": "e8a1b7d5-c153-4a6d-ca13-68dcf5ff6bc8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-08-04 12:26 /word_count_with_python/output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup     233636 2023-08-04 12:26 /word_count_with_python/output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHLP2RKmhRVf",
        "outputId": "273e0486-f4df-4a67-f63a-a3efe41993d3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"(d)\t1\n",
            "\"/H\"\t1\n",
            "\"0\"\t1\n",
            "\"02\"\t2\n",
            "\"1.\t1\n",
            "\"414\t3\n",
            "\"A\t2\n",
            "\"AT&T\t3\n",
            "\"AT&T's\t1\n",
            "\"Access\t1\n",
            "\"Acid\t3\n",
            "\"Ad-hocracy\"\t1\n",
            "\"Advanced\t1\n",
            "\"Agents\t1\n",
            "\"Al\t3\n",
            "\"All\t3\n",
            "\"American\t1\n",
            "\"An\t1\n",
            "\"And\t2\n",
            "\"Any\t1\n",
            "\"Are\t3\n",
            "\"Artificial\t1\n",
            "\"As\t1\n",
            "\"Assistant\t1\n",
            "\"Attctc\"\t2\n",
            "\"Auld\t1\n",
            "\"Autodesk,\"\t1\n",
            "\"BBS,\"\t2\n",
            "\"BIRTHPLACE\t1\n",
            "\"BRITS\t1\n",
            "\"Barry\t1\n",
            "\"Because\t1\n",
            "\"Before\t1\n",
            "\"Bell\t2\n",
            "\"Bell\"\t1\n",
            "\"BellSouth\t2\n",
            "\"Berkeley\t1\n",
            "\"Big\t1\n",
            "\"Biggest\t1\n",
            "\"Black\t3\n",
            "\"Blue\t1\n",
            "\"Bob\"\t1\n",
            "\"Bob,\t3\n",
            "\"Bob:\t1\n",
            "\"Bullet-N-Board.\"\t1\n",
            "\"Bureaucrat-ese\t1\n",
            "\"But\t4\n",
            "\"C-word.\"\t1\n",
            "\"CALIFORNIA\"\t1\n",
            "\"CC,\"\t1\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_aphPXvfgMzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dF4T1tFGcZ4m"
      }
    }
  ]
}