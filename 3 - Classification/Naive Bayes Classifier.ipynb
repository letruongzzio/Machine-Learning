{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that the output is class $c$ given that the input is vector $\\textbf{x}\\in\\mathbb{R}^d$ is $\\mathbb{P}(c|\\textbf{x})$. This expression, if calculated, can help determine the class of each data point by selecting the class with the highest probability of falling into:\n",
    "\n",
    "$$c = {\\underset{c\\in\\{1, \\ldots, C\\}}{\\operatorname{arg\\,max}}}\\mathbb{P}(c|\\textbf{x}) = {\\underset{c\\in\\{1, \\ldots, C\\}}{\\operatorname{arg\\,max}}}\\frac{\\mathbb{P}(\\textbf{x}|c)}{\\mathbb{P}(\\textbf{x})} = {\\underset{c\\in\\{1, \\ldots, C\\}}{\\operatorname{arg\\,max}}}\\mathbb{P}(\\textbf{x}|c)\\mathbb{P}(c)$$\n",
    "\n",
    "The third \"=\" occurs because $\\mathbb{P}(\\textbf{x})$ does not depend on $c$. If the training set is large, it can be determined using the maximum likelihood estimator (MSE). Conversely, we can use maximum a posteriori (MAP).\n",
    "\n",
    "The name Naive Bayes Classifier was born from the assumption of independence of the high-dimensional random variable $\\textbf{x}$. Although this assumption is too strict and in reality it is difficult to find data that are completely independent of each other, this assumption sometimes brings unexpected results. Where $c$ is a known value:\n",
    "\n",
    "$$\\mathbb{P}(\\textbf{x}|c) = \\mathbb{P}(x_1, x_2, \\ldots, x_d|c) = \\prod_{i=1}^{d}\\mathbb{P}(x_i|c)$$\n",
    "\n",
    "Then we get:\n",
    "\n",
    "$$c = {\\underset{c\\in\\{1, \\ldots, C\\}}{\\operatorname{arg\\,max}}}\\mathbb{P}(c)\\prod_{i=1}^{d}\\mathbb{P}(x_i|c)$$\n",
    "\n",
    "When $d$ is large and the probabilities are small, the expression on the right-hand side of $c$ will be a very small number that is influential and produces errors. To handle this, $c$ is often rewritten in equivalent form by taking $\\log$ both sides of the right-hand side. This does not affect the result because $\\log$ is a uniform function on the set of positive numbers.\n",
    "\n",
    "Commonly used distributions in NBC:\n",
    "\n",
    "+ Gaussian naive Bayes: Consider $\\{x_i\\} \\sim \\mathcal{N}(\\mu_{c_i}, \\sigma_{c_i}^2).$ In which, the parameter set $\\theta = \\{\\mu_{c_i}, \\sigma_{c_i}^2\\}$ is determined by MLE based on the points in the training set belonging to class $c$:\n",
    "\n",
    "$$\\mathbb{P}(x_i|c) = \\mathbb{P}(x_i|\\mu_{c_i}, \\sigma_{c_i}^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_{c_i}^2}}\\exp{\\left(-\\frac{(x_i - \\mu_{c_i})^2}{2\\sigma_{c_i}^2}\\right)}$$\n",
    "\n",
    "+ Multinomial naive Bayes: This is the model mainly used in text classification. First of all, we will give the formula:\n",
    "\n",
    "$$\\lambda_{c_i} = \\mathbb{P}(x_i|c) = \\frac{N_{c_i}}{N_c}$$\n",
    "\n",
    "In there:\n",
    "\n",
    "\\+ $N_{c_i}$ is the total number of times the word $i$ appears in documents of class $c$. Or we can say that it is the sum of all the $i$ features of the feature vectors corresponding to class $c$.\n",
    "\n",
    "\\+ $N_c$ is the total number of words appearing in class $c$ or the length of class $c$. It can be deduced that $N_c = \\sum_{i=1}^{d}N_{c_i}$ and $\\sum_{i=1}^d\\lambda_{c_i} = 1$ where $d$ is the number of words in dictionary.\n",
    "\n",
    "This calculation has a limitation: if there is a new word that has never appeared in class $c$, the expression $\\lambda_{c_i}$ will be zero, leading to the expression $c$ being zero. To solve this problem, a *Laplace smoothing* technique is used:\n",
    "\n",
    "$$\\hat{\\lambda_{c_i}} = \\frac{N_{c_i} + \\alpha}{N_c + d\\alpha}$$\n",
    "\n",
    "Where $\\alpha$ is a positive number (usually equal to 1). The denominator is added to $d\\alpha$ to ensure the total probability $\\sum_{i=1}^d\\hat{\\lambda_{c_i}} = 1$. So that each class $c$ will be described by a set of positive numbers whose sum is 1: $\\hat{\\lambda_c} = \\{\\hat{\\lambda_{c_1}}, \\hat{\\lambda_{c_2}}, \\ldots, \\hat{\\lambda_{c_d}}\\}$.\n",
    "\n",
    "+ Bernoulli Naive Bayes: This model is applicable to data types where each element is a binary value of 0 or 1. For example, also with text types but instead of counting the total number of occurrences of a word in the text, We just need to care whether that word appears or not:\n",
    "\n",
    "$$\\mathbb{P}(x_i|c) = \\mathbb{P}(i|c)x_i + [1 - \\mathbb{P}(i|c)](1 - x_i)$$\n",
    "\n",
    "With $\\mathbb{P}(x_i|c)$ it can be understood as the probability that the word $i$ appears in the text of class $c$.\n",
    "\n",
    "Based on the example in document $[1]$, *pages 130-131*, we will recalculate using the available `scikit-learn` library. Note that $\\mathbb{P}(B|d5)$ and $\\mathbb{P}(N|d5)$ are only proportional to $\\mathbb{P}(B)\\prod_{i=1}^d\\mathbb{P}(x_i|B)$ and $\\mathbb{P}(N)\\prod_{i=1}^d\\mathbb{P}(x_i|N)$ because we ignored $\\mathbb{P} (\\textbf{x})$ as mentioned before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Multinomial naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting class of d5: B\n",
      "Probability of d6 in each class: [0.29175335 0.70824665]\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "d1 = [2, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "d2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "d3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "d4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "train_data = np.array([d1, d2, d3, d4])\n",
    "label = np.array(['B', 'B', 'B', 'N']) \n",
    "\n",
    "# test data\n",
    "d5 = np.array([[2, 0, 0, 1, 0, 0, 0, 1, 0]])\n",
    "d6 = np.array([[0, 1, 0, 0, 0, 0, 0, 1, 1]])\n",
    "\n",
    "# call MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "# training \n",
    "mnb.fit(train_data, label)\n",
    "\n",
    "# test\n",
    "print('Predicting class of d5:', str(mnb.predict(d5)[0]))\n",
    "print('Probability of d6 in each class:', mnb.predict_proba(d6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Bernoulli naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting class of d5: B\n",
      "Probability of d6 in each class: [0.16948581 0.83051419]\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "d1 = [1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "d2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "d3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "d4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "train_data = np.array([d1, d2, d3, d4])\n",
    "label = np.array(['B', 'B', 'B', 'N'])\n",
    "\n",
    "# test data\n",
    "d5 = np.array([[1, 0, 0, 1, 0, 0, 0, 1, 0]])\n",
    "d6 = np.array([[0, 1, 0, 0, 0, 0, 0, 1, 1]])\n",
    "\n",
    "## call MultinomialNB\n",
    "clf = BernoulliNB()\n",
    "# training \n",
    "clf.fit(train_data, label)\n",
    "\n",
    "# test\n",
    "print('Predicting class of d5:', str(clf.predict(d5)[0]))\n",
    "print('Probability of d6 in each class:', clf.predict_proba(d6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Naive Bayes Classifier to classify spam emails.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix # # check this: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html for more information about coo_matrix function \n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score # for evaluating results\n",
    "\n",
    "# data path and file name \n",
    "path = 'ex6DataPrepared/'\n",
    "train_data_fn = 'train-features.txt'\n",
    "test_data_fn = 'test-features.txt'\n",
    "train_label_fn = 'train-labels.txt'\n",
    "test_label_fn = 'test-labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords = 2500 \n",
    "\n",
    "def read_data(data_fn, label_fn):\n",
    "    ## read data_fn\n",
    "    with open(path + data_fn) as f:\n",
    "        content = f.readlines() # read all lines in the file\n",
    "    content = [x.strip() for x in content] # remove '\\n' at the end of each line\n",
    "\n",
    "    ## read label_fn\n",
    "    with open(path + label_fn) as f:\n",
    "        label = f.readlines()\n",
    "    label = [int(x.strip()) for x in label]\n",
    "\n",
    "    dat = np.zeros((len(content), 3), dtype=int)\n",
    "    for i, line in enumerate(content): \n",
    "        a = line.split(' ')\n",
    "        dat[i, :] = np.array([int(a[0]), int(a[1]), int(a[2])])\n",
    "    \n",
    "    # remember to -1 at coordinate since we're in Python\n",
    "    data = coo_matrix((dat[:, 2], (dat[:, 0] - 1, dat[:, 1] - 1)), shape=(len(label), nwords))\n",
    "    \n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18)\t2\n",
      "  (0, 44)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 84)\t1\n",
      "  (0, 138)\t1\n",
      "  (0, 199)\t1\n",
      "  (0, 350)\t1\n",
      "  (0, 351)\t1\n",
      "  (0, 512)\t1\n",
      "  (0, 563)\t1\n",
      "  (0, 742)\t1\n",
      "  (0, 776)\t1\n",
      "  (0, 1130)\t1\n",
      "  (0, 1276)\t1\n",
      "  (0, 1638)\t1\n",
      "  (0, 1763)\t1\n",
      "  (0, 1815)\t1\n",
      "  (0, 1867)\t1\n",
      "  (1, 34)\t5\n",
      "  (1, 96)\t1\n",
      "  (1, 102)\t2\n",
      "  (1, 158)\t1\n",
      "  (1, 293)\t2\n",
      "  (1, 726)\t1\n",
      "  :\t:\n",
      "  (699, 2048)\t4\n",
      "  (699, 2054)\t4\n",
      "  (699, 2057)\t2\n",
      "  (699, 2071)\t4\n",
      "  (699, 2084)\t2\n",
      "  (699, 2108)\t3\n",
      "  (699, 2126)\t1\n",
      "  (699, 2172)\t1\n",
      "  (699, 2198)\t3\n",
      "  (699, 2226)\t1\n",
      "  (699, 2231)\t1\n",
      "  (699, 2236)\t1\n",
      "  (699, 2244)\t1\n",
      "  (699, 2325)\t1\n",
      "  (699, 2356)\t3\n",
      "  (699, 2377)\t2\n",
      "  (699, 2397)\t4\n",
      "  (699, 2401)\t1\n",
      "  (699, 2418)\t1\n",
      "  (699, 2432)\t1\n",
      "  (699, 2433)\t1\n",
      "  (699, 2471)\t1\n",
      "  (699, 2478)\t2\n",
      "  (699, 2480)\t2\n",
      "  (699, 2499)\t3\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Training size = 700, accuracy = 98.08%\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_label) = read_data(train_data_fn, train_label_fn)\n",
    "(test_data, test_label)  = read_data(test_data_fn, test_label_fn)\n",
    "\n",
    "print(train_data, train_label, sep='\\n')\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_data, train_label)\n",
    "\n",
    "y_pred = clf.predict(test_data)\n",
    "print('Training size = %d, accuracy = %.2f%%' % (train_data.shape[0], accuracy_score(test_label, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue trying with smaller training data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size = 100, accuracy = 97.69%\n"
     ]
    }
   ],
   "source": [
    "train_data_fn = 'train-features-100.txt'\n",
    "train_label_fn = 'train-labels-100.txt'\n",
    "test_data_fn = 'test-features.txt'\n",
    "test_label_fn = 'test-labels.txt'\n",
    "\n",
    "(train_data, train_label)  = read_data(train_data_fn, train_label_fn)\n",
    "(test_data, test_label)  = read_data(test_data_fn, test_label_fn)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_data, train_label)\n",
    "y_pred = clf.predict(test_data)\n",
    "print('Training size = %d, accuracy = %.2f%%' % (train_data.shape[0],accuracy_score(test_label, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size = 50, accuracy = 97.31%\n"
     ]
    }
   ],
   "source": [
    "train_data_fn = 'train-features-50.txt'\n",
    "train_label_fn = 'train-labels-50.txt'\n",
    "test_data_fn = 'test-features.txt'\n",
    "test_label_fn = 'test-labels.txt'\n",
    "\n",
    "(train_data, train_label)  = read_data(train_data_fn, train_label_fn)\n",
    "(test_data, test_label)  = read_data(test_data_fn, test_label_fn)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_data, train_label)\n",
    "y_pred = clf.predict(test_data)\n",
    "print('Training size = %d, accuracy = %.2f%%' % (train_data.shape[0],accuracy_score(test_label, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size = 50, accuracy = 69.62%\n"
     ]
    }
   ],
   "source": [
    "# trying with `BernoulliNB` model\n",
    "clf = BernoulliNB()\n",
    "clf.fit(train_data, train_label)\n",
    "y_pred = clf.predict(test_data)\n",
    "print('Training size = %d, accuracy = %.2f%%' % (train_data.shape[0],accuracy_score(test_label, y_pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
